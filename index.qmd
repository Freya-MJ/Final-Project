---
title: "Data Science for Public Policy"
subtitle: "Final Project"
author: "Zehui Li Lezhi Cao, Mujin Li, Xinwen Zhang"
execute:
  warning: false
format:
  html:
    embed-resources: true
editor: visual
---

## library packages

```{r}
library(tidyverse)
library(lubridate)
library(tidymodels)
library(themis)
library(recipes)
library(parsnip)
library(ranger)
library(ggplot2)
library(vip)
library(patchwork)
library(stringr)
library(tidyclust)
library(tidymodels)
library(Rfast)
library(viridis)
library(ggthemes)
library(hrbrthemes)
```

## Machine learning

### data preparation

```{r}
data<-read_csv("/Users/macbookpro/Desktop/finalproj/Final-Project/data_finalproject.csv")

povertynew<-read_excel("/Users/macbookpro/Desktop/finalproj/Final-Project/povertyrate.xlsx")

povertynew<-povertynew%>%
  pivot_longer(!name,
               names_to = "year",
               values_to = "poverty")

povertynew <- povertynew %>% 
  mutate(year = as.double(year))%>%
  rename("povertyrate"="poverty")

povertyold<-data%>%
   select("name","year","percent_in_poverty")%>%
  rename("povertyrate"="percent_in_poverty")

povertyold<-povertyold%>%
  filter(year<2019)

poverty<-bind_rows(povertynew,povertyold,id=NULL)

data_clean<-left_join(data,poverty,by=c("name","year"))

hs<-read_csv("/Users/macbookpro/Desktop/finalproj/Final-Project/hs.csv")%>%
  select(-1,-2)%>%
  rename("name"="state")

data_clean<-left_join(data_clean,hs,by=c("name","year"))

data_clean<-data_clean%>%
  select(-"percent_in_poverty",-"data_totals_violent_all",-"data_totals_property_all",-"number_in_poverty")

data_clean%>%
  group_by(name)%>%
  summarise(missing_values = sum(is.na(poverty)))

data_clean <- data_clean %>%
  mutate_all(~ replace(., is.na(.), 0))

data_clean<-data_clean%>%
  select(-1)

implement<-data_clean%>%
  filter(year>2021)

data_clean<-data_clean%>%
  filter(year<2022)

write.csv(data_clean, file = "/Users/macbookpro/Desktop/finalproj/Final-Project/data_clean.csv", row.names = FALSE)
```

### Unsupervised machine learning: Kmeans

```{r}
#resample
set.seed(1214)
kmeans_cv <- vfold_cv(data_clean, v = 3)
```

#### Build the model

```{r}
# Create the recipe
kmeans_rec<-recipe(~., data = data_clean)%>%
  update_role(name,abbreviation,new_role = "ID")%>%
  step_dummy(all_nominal_predictors())%>%
  step_normalize(all_predictors())%>%
  step_pca(all_numeric(), threshold = .90)%>%
  prep()

kmeans<-bake(kmeans_rec, new_data = data_clean)
# build the model
kmeans_spec <- k_means(
  num_clusters = tune()
) %>%
  set_engine(
    "stats",
    nstart = 100 
  )
#Create the workflow
kmeans_wflow <- workflow(
  preprocessor = kmeans_rec,
  spec = kmeans_spec
)

# Create the grid for tuning
clust_num_grid <- grid_regular(
  num_clusters(), 
  levels = 10
)
# tune the k-means clustering algorithm
res <- tune_cluster(
  object = kmeans_wflow,
  resamples = kmeans_cv,
  grid = clust_num_grid,
  control = control_grid(save_pred = TRUE),
  metrics = cluster_metric_set(sse_within_total,silhouette_avg)
)
```

#### Evaluate the Model

```{r}
wss<-res%>%
  collect_metrics() %>%
  filter(.metric == "sse_within_total")
wss

silhouette<-res%>%
  collect_metrics() %>%
  filter(.metric == "silhouette_avg")
silhouette

# select by plot
wss_plot<-res %>%
  collect_metrics() %>%
  filter(.metric == "sse_within_total") %>%
  ggplot(aes(x = num_clusters, y = mean)) +
  geom_point() + 
  geom_line() + 
  scale_x_continuous(breaks = 1:10) +
  labs(
    x = "Number of clusters",
    y = "mean WSS over 5 folds"
  ) +
  theme_minimal() 
wss_plot
silhouette<-res%>%
  collect_metrics() %>%
  filter(.metric == "silhouette_avg") %>%
  ggplot(aes(x = num_clusters, y = mean)) +
  geom_point() + 
  geom_line() + 
  scale_x_continuous(breaks = 1:10) +
  labs(
    x = "Number of clusters",
    y = "mean silhouette_avg over 5 folds"
  ) +
  theme_minimal() 
silhouette
#optimal cluster for wss is 4 and for silhouette is 4
```

#### Model Viz

```{r}
kmean_plot<-bake(kmeans_rec,new_data = data_clean)%>%
  select("PC1","PC2")

kmeans_spec_final <- k_means(
  num_clusters = 4 # number of clusters
) %>%
  set_engine(
    "stats",
    nstart = 100 # number of random starts
  )

# create the final workflow
kmeans_wflow_final <- workflow(
  preprocessor =kmeans_rec,
  spec = kmeans_spec_final
)

# fit the final model
final_fit <- fit(
  kmeans_wflow_final,
  data = data_clean
)

tidy(final_fit)

clusters <- bind_cols(
  final_fit %>%
    extract_recipe() %>%
    bake(data_clean),
  cluster = final_fit %>%
    extract_cluster_assignment() %>%
    pull(.cluster)
)
cluster_plot <- ggplot() +
  geom_point(data = clusters, mapping = aes(x = PC1, y = PC2,color = factor(cluster)))+
  labs(title = "K-Means with K = 4 and PCA")+ 
  # use paste to dynamically include the value of k in the title
  theme_minimal()

cluster_plot
```

### Supervised machine learning

#### linear regression: ridge

```{r}
#plit the data into training and testing sets 
set.seed(12143)
split<-initial_split(data_clean, 
  prop = 0.7, 
  strata = "death_rate"
)
train <- training(split)
test <- testing(split)
```

```{r}
#resample
set.seed(1234)
folds <- vfold_cv(data = train, v = 5)
```

##### Build the model

```{r}
# create a recipe
ridge_rec <- 
  recipe(death_rate ~ ., data = data_clean) %>%
  update_role(name,abbreviation,new_role = "ID")%>%
  # drop near zero variance predictors
  step_nzv(all_predictors()) %>%
  # center and scale predictors
  step_center(all_predictors()) %>%
  step_scale(all_predictors())%>%
  prep()
```

```{r}
# create a tuning grid for ridge regularization, varying the regularization penalty
ridge_grid <- grid_regular(penalty(), levels = 10)

# create a linear_regression model with tuned the penalty parameter
ridge_mod <- linear_reg(
  penalty = tune(), 
  mixture = 1
) %>%
  set_engine("glmnet")

# create a ridge workflow 
ridge_wf <- workflow() %>%
  add_recipe(ridge_rec) %>%
  add_model(ridge_mod)

# perform hyperparameter tuning using the on ridge hyperparameter grid and cross_validation folds 
ridge_cv <- ridge_wf %>%
  tune_grid(
    resamples = folds,
    grid = ridge_grid,
    control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse)
  )

# select the best model based on the "rmse" metric
ridge_best <- ridge_cv %>%
  select_best(metric = "rmse")

# fit the final ridge model to the full training data and extract coefficients
ridge_final <- finalize_workflow(
  ridge_wf,
  parameters = ridge_best
)
# by updating the line below
ridge_coefs <- ridge_final %>%
  fit(data = train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = ridge_best$penalty) 
# colculate the importance score
ridge_importance<-ggplot(data=ridge_coefs)+
  geom_bar(aes(x=reorder(Variable, Importance),y = Importance),stat = "identity")+
  coord_flip()+
  labs(title = "Importance Score(Ridge with Penalty)", x = "Variable", y = "Important Score")+
  theme_minimal()
ridge_importance
```

##### Evaluate the model

```{r}
# plot the visulization of penalty and rmse
ridge_rmse<-
  collect_metrics(ridge_cv, summarize = FALSE) %>% 
  filter(.metric == "rmse") 

ridge_rmse_plot <-ggplot(ridge_rmse, aes(x = penalty, y = .estimate,color = id)) +
  geom_line() +
  geom_point() +
  labs(title = "RMSE Across Ridge Regression Resamples",
       x = "Penalty",
       y = "RMSE") +
  theme_minimal()

ridge_rmse_plot
```

##### Model viz

```{r}
# use best model make predictions with test dataset
ridge_best_test <- finalize_workflow(
  ridge_wf,
  parameters = ridge_best
)
ridge_best_test_fit<-ridge_best_test%>%
  fit(data = train) # use original train dataset

ridge_prediction <- ridge_best_test_fit%>%
  predict(new_data = test) #use test dataset

ridge_rmse_test <- bind_cols(test %>% select(death_rate),  
                       ridge_prediction %>% select(.pred)
                       )%>% #combine the true value from test dataset and the predicted value
  rmse(truth = death_rate, estimate = .pred)
ridge_rmse_test
ridge_predict2022 <- ridge_best_test_fit%>%
  predict(new_data = implement)
ridge_predict2022
```

#### Random Forest

##### Build the model

```{r}
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>% 
  set_engine("ranger",num.threads = 10) %>% 
  set_mode("regression")
#create the workflow
rf_wf <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(ridge_rec)
```

##### Evaluate the model: RMSE & out of sample rate

```{r}
# train and tune the model
set.seed(345)
rf_res <- 
  rf_wf %>% 
  tune_grid(folds,
            grid = 5,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
# find the best mtry&min_n value
rf_res %>% 
  show_best(metric = "rmse")

# plot the visulization of mtry and rmse
rf_rmse<-
  collect_metrics(rf_res, summarize = FALSE) %>% 
  filter(.metric == "rmse") 

rf_rmse_plot <-ggplot(rf_rmse, aes(x = mtry, y = .estimate,color = id)) +
  geom_line() +
  geom_point() +
  labs(title = "RMSE Across Random Forest Resamples",
       x = "mtry",
       y = "RMSE") +
  theme_minimal()

rf_rmse_plot 
#select my best random forest model
rf_best<-rf_res%>%
  select_best(metric = "rmse")
rf_best
# use best model make predictions with test dataset
rf_best_test <- finalize_workflow(
  rf_wf,
  parameters = rf_best
)
rf_best_test_fit<-rf_best_test%>%
  fit(data = train) # use original train dataset
test_prediction <- rf_best_test_fit%>%
  predict(new_data = test) #use test dataset

rmse_test <- bind_cols(test %>% select(death_rate),  
                       test_prediction %>% select(.pred)
                       )%>% #combine the true value from test dataset and the predicted value
  rmse(truth = death_rate, estimate = .pred)
rmse_test
```

##### predict result

```{r}
predict2022 <- rf_best_test_fit%>%
  predict(new_data = implement)
predict2022$.pred
```

##### Model copare

```{r}
predict_compare<-bind_cols(predict2022,ridge_predict2022)

name <- data_clean %>%
  distinct(name) %>%
  arrange(row_number())

predict_compare<-bind_cols(name,predict_compare)

colnames(predict_compare)[2] <- "Ridge"

colnames(predict_compare)[3] <- "Random Forest"
  
predict_compare_long<-predict_compare%>%
  pivot_longer(!name,
               names_to = "model",values_to = "prediction")

states_data <- tibble(
  state = c("Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", "Florida", "Georgia", 
            "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", 
            "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada", "New Hampshire", 
            "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", 
            "Rhode Island", "South Carolina", "South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington", 
            "West Virginia", "Wisconsin", "Wyoming", "District of Columbia"),
  region = c("South", "West", "West", "South", "West", "West", "Northeast", "South", "South", "South", 
             "West", "West", "Midwest", "Midwest", "Midwest", "Midwest", "South", "South", "Northeast", "South", 
             "Northeast", "Midwest", "Midwest", "South", "Midwest", "West", "Midwest", "West", "Northeast", "Northeast", 
             "Northeast", "West", "Northeast", "South", "Midwest", "Midwest", "West", "Northeast", "South", "West", 
             "Northeast", "South", "Midwest", "South", "West", "Northeast", "South", "West", "South", "Midwest", "West")
)
```

```{r}

states_data<-states_data%>%
  rename("name"="state")

outcome<-left_join(predict_compare_long,states_data,by ="name")

abbreviate<-data_clean%>%
  select("abbreviation","name")

outcome<-left_join(outcome,abbreviate,by="name")

outcome_plot<-ggplot(data = outcome)+
   geom_point(aes(x=abbreviation,y=prediction,color = model, shape = region),
    size = 1.5, 
    alpha = 0.8,na.rm = TRUE)+
    facet_wrap(~ region,scales = "free_x")+
   labs(title ="Predict 2022 Death Rate by Region", x= "State",y="Prediction",subtitle = "Source: CDC/National Center for Health Statistics",caption = "The number of deaths per 100,000 total population.")+
   theme_ipsum()+
   theme(axis.text.x = element_text(size = 8),
        strip.text.x = element_text(size = 10,face="bold"),
        plot.background = element_rect(color = "#fef6e4"),
        plot.title = element_text(face = "bold"))
outcome_plot
```
